{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Part 4 (Model V1)\n",
    "\n",
    "In this part, we will explore the behavior of uncertainty (14 day STD) on individual stocks. So, what you should expect is a lot of \"generic\" code for processing the whole folders (35 files) because we cannot do it by hand unfortunately! Specifically, we will do multiple linear regression of future 14 day STD on three independent variables: past 14 day STD (lagged variable), (price - prev 14 day MA)^2 (error squared), error_squared *indicator r.v. (positive/negative shock)*. As you can notice, this is similar to GJR Garch, but since I think scikit-learn package is more feasible to do \"generic\" coding, so I will use my own proxy for variance (STD-14). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRICE_FILES = [\"data/Stocks/AlternativeEnergy_Price.csv\", \"data/Stocks/Automobile_Price.csv\", \"data/Stocks/Bank_Price.csv\",\n",
    "                        \"data/Stocks/Beverage_Price.csv\", \"data/Stocks/BioTech_Price.csv\", \"data/Stocks/Chemical_Price.csv\",\n",
    "                        \"data/Stocks/Construction_Price.csv\", \"data/Stocks/Electricity_Price.csv\", \"data/Stocks/Electronic_Price.csv\",\n",
    "                        \"data/Stocks/Finance_Price.csv\", \"data/Stocks/Fix_Price.csv\", \"data/Stocks/Food_Price.csv\", \n",
    "                        \"data/Stocks/FoodProducer_Price.csv\", \"data/Stocks/Gas_Price.csv\", \"data/Stocks/GeneralIndustrial_Price.csv\",\n",
    "                        \"data/Stocks/GeneralRetail_Price.csv\", \"data/Stocks/Hardware_Price.csv\", \"data/Stocks/Health_Price.csv\",\n",
    "                        \"data/Stocks/Household_Price.csv\", \"data/Stocks/IndustrialEngineer_Price.csv\", \"data/Stocks/IndustrialMetal_Price.csv\", \n",
    "                        \"data/Stocks/IndustrialTransport_Price.csv\", \"data/Stocks/Insurance_Price.csv\", \"data/Stocks/Leisure_Price.csv\",\n",
    "                        \"data/Stocks/Media_Price.csv\", \"data/Stocks/Mining_Price.csv\", \"data/Stocks/NonLifeInsure_Price.csv\",\n",
    "                        \"data/Stocks/OilProducer_Price.csv\", \"data/Stocks/Paper_Price.csv\", \"data/Stocks/PersonalGoods_Price.csv\",\n",
    "                        \"data/Stocks/RealEstate_Price.csv\", \"data/Stocks/Software_Price.csv\", \"data/Stocks/Support_Price.csv\",\n",
    "                        \"data/Stocks/Travel_Price.csv\", \"data/Stocks/Unclassified_Price.csv\"]\n",
    "\n",
    "VOLUME_FILES = [\"data/Stocks/AlternativeEnergy_Volume.csv\", \"data/Stocks/Automobile_Volume.csv\", \"data/Stocks/Automobile_Volume.csv\",\n",
    "                           \"data/Stocks/Beverage_Volume.csv\", \"data/Stocks/BioTech_Volume.csv\", \"data/Stocks/Chemical_Volume.csv\",\n",
    "                           \"data/Stocks/Construction_Volume.csv\", \"data/Stocks/Electricity_Volume.csv\", \"data/Stocks/Electronic_Volume.csv\",\n",
    "                           \"data/Stocks/Finance_Volume.csv\", \"data/Stocks/Fix_Volume.csv\", \"data/Stocks/Food_Volume.csv\", \n",
    "                           \"data/Stocks/FoodProducer_Volume.csv\", \"data/Stocks/Gas_Volume.csv\", \"data/Stocks/GeneralIndustrial_Volume.csv\",\n",
    "                           \"data/Stocks/GeneralRetail_Volume.csv\", \"data/Stocks/Hardware_Volume.csv\", \"data/Stocks/Health_Volume.csv\", \n",
    "                           \"data/Stocks/Household_Volume.csv\", \"data/Stocks/IndustrialEngineer_Volume.csv\", \"data/Stocks/IndustrialMetal_Volume.csv\", \n",
    "                           \"data/Stocks/IndustrialTransport_Volume.csv\", \"data/Stocks/Insurance_Volume.csv\", \"data/Stocks/Leisure_Volume.csv\",\n",
    "                           \"data/Stocks/Media_Volume.csv\", \"data/Stocks/Mining_Volume.csv\", \"data/Stocks/NonLifeInsure_Volume.csv\",\n",
    "                           \"data/Stocks/OilProducer_Volume.csv\", \"data/Stocks/Paper_Volume.csv\", \"data/Stocks/PersonalGoods_Volume.csv\",\n",
    "                           \"data/Stocks/RealEstate_Volume.csv\", \"data/Stocks/Software_Volume.csv\", \"data/Stocks/Support_Volume.csv\",\n",
    "                           \"data/Stocks/Travel_Volume.csv\", \"data/Stocks/Unclassified_Volume.csv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RenameHeader(col_name, col_type):\n",
    "    if col_name == \"Code\":\n",
    "        return \"Code\"\n",
    "    else:\n",
    "        if col_type == \"Price\":\n",
    "            return col_name[2:-3]\n",
    "        else:\n",
    "            return col_name[2:-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Clean(price, volume):\n",
    "    \n",
    "    \"\"\"\n",
    "    Argument: price dataframe and volume dataframe\n",
    "    Return: return the tuples of price dataframes (2014-2019) and volume dataframes (2014-2019)\n",
    "                where each one of them is filtered out missing col, missing price from prev year, missing vol from prev year,\n",
    "                        no volume traded occurs from prev year\n",
    "    Note: this function requires RenameHeader (Defined above) and pandas\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read two dataframes and filter to have files from 2014 to 2019\n",
    "    price_df = pd.read_csv(price).rename(columns = lambda x: RenameHeader(x, \"Price\"))\n",
    "    price_df['Code'] = pd.to_datetime(price_df['Code'])\n",
    "    price_df = price_df[(price_df['Code'] > '2014-01-01') & (price_df['Code'] < '2020-01-01')]\n",
    "    \n",
    "    vol_df = pd.read_csv(volume).rename(columns = lambda x: RenameHeader(x, \"Vol\"))\n",
    "    vol_df['Code'] = pd.to_datetime(vol_df['Code'])\n",
    "    vol_df = vol_df[(vol_df['Code'] > '2014-01-01') & (vol_df['Code'] < '2020-01-01')]\n",
    "    \n",
    "    # Use only common cols in two dataframes\n",
    "    common_cols = price_df.columns.intersection(vol_df.columns).tolist()\n",
    "    price_df = price_df[common_cols]\n",
    "    vol_df = vol_df[common_cols]\n",
    "    \n",
    "    # Temporarily include SET_VOL to filter holidays\n",
    "    SET_IDX_VOL = pd.read_csv('data/SET/SET_VO.csv', parse_dates = True)\n",
    "    SET_IDX_VOL = SET_IDX_VOL.rename(columns = {'Code': 'Code', 'BNGKSET(VO)': 'Volume'})\n",
    "    SET_IDX_VOL['Code'] = pd.to_datetime(SET_IDX_VOL['Code'])\n",
    "    \n",
    "    # Filter holiday on price dataframes\n",
    "    price_df = pd.merge(price_df, SET_IDX_VOL, how = 'inner', on = 'Code')\n",
    "    price_df = price_df[price_df['Volume'].notna()]\n",
    "    price_df.drop(['Volume'], axis = 1, inplace = True)\n",
    "    \n",
    "    # Filter holiday on price dataframes\n",
    "    vol_df = pd.merge(vol_df, SET_IDX_VOL, how = 'inner', on = 'Code')\n",
    "    vol_df = vol_df[vol_df['Volume'].notna()]\n",
    "    vol_df.drop(['Volume'], axis = 1, inplace = True)\n",
    "    \n",
    "    # loop from 2014 to 2019 to create small dataframes (useful for filtering later on)\n",
    "    price_df_s = []\n",
    "    vol_df_s = []\n",
    "    for i in range(6):\n",
    "        x = i + 2014\n",
    "        start_date = str(x) + '-01-01'\n",
    "        end_date = str(x+1) + '-01-01'\n",
    "        price_df_s.append(price_df[(price_df['Code'] > start_date) & (price_df['Code'] < end_date)])\n",
    "        vol_df_s.append(vol_df[(vol_df['Code'] > start_date) & (vol_df['Code'] < end_date)])\n",
    "        \n",
    "    # Filter the columns from 2015 to 2019 by two criterias\n",
    "    # 1) Any missing price variables from previous year\n",
    "    # 2) Any missing + zero volume from previous year\n",
    "    for i in range(5):\n",
    "        \n",
    "        price_filter_df = price_df_s[i]\n",
    "        vol_filter_df = vol_df_s[i]\n",
    "        \n",
    "        price_null = price_filter_df.columns[price_filter_df.isna().any()].tolist()\n",
    "        vol_null = vol_filter_df.columns[vol_filter_df.isna().any()].tolist()\n",
    "        # Create another copy (a bit inefficient, but it works)\n",
    "        vol_temp = vol_filter_df.drop(['Code'], axis = 1, inplace = False)\n",
    "        vol_gt_zero = vol_temp.columns[(vol_temp <= 0).any()].tolist()\n",
    "        \n",
    "        filtered_out_col = list(set().union(price_null,vol_null,vol_gt_zero))\n",
    "        \n",
    "        price_df_s[i+1].drop(filtered_out_col, axis = 1, inplace = True)\n",
    "        vol_df_s[i+1].drop(filtered_out_col, axis = 1, inplace = True)\n",
    "        \n",
    "        \n",
    "    return (price_df_s, vol_df_s)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateFeatures(price_list, vol_list):\n",
    "    \"\"\"\n",
    "    Input: price dataframes list, vol dataframes list\n",
    "    Return: list of list of dict\n",
    "    (for example, [automobile, ...] -> [2014, 2015, 2016, ...] -> {price: price_df, vol: vol_df, feature1: ...})\n",
    "    \"\"\"\n",
    "    return_list = []\n",
    "    for i in range(len(price_list)):\n",
    "        compile_list = []\n",
    "        price_df_s, vol_df_s = Clean(price_list[i], vol_list[i])\n",
    "        \n",
    "        for j in range(len(price_df_s)):\n",
    "            sub_dict = {}\n",
    "            price_df = price_df_s[j]\n",
    "            vol_df = vol_df_s[j]\n",
    "            \n",
    "            ## Define new dataframes: this records the 14-day variance forward (dependent variables)\n",
    "            y_df = pd.DataFrame(index=price_df.index, columns=price_df.columns)\n",
    "            y_df['Code'] = price_df['Code']\n",
    "            ## Define new dataframes: lagged dependent variable: previous 14-day variance forward\n",
    "            y_lag_df = pd.DataFrame(index=price_df.index, columns=price_df.columns)\n",
    "            y_lag_df['Code'] = price_df['Code']\n",
    "            ## Define new dataframes: this records error square\n",
    "            error_sq_df = pd.DataFrame(index=price_df.index, columns=price_df.columns)\n",
    "            error_sq_df['Code'] = price_df['Code']\n",
    "            ## Define new dataframes: this records indicator * error_square\n",
    "            error_ind_df = pd.DataFrame(index=price_df.index, columns=price_df.columns)\n",
    "            error_ind_df['Code'] = price_df['Code']            \n",
    "            \n",
    "            \n",
    "            for col in list(price_df.columns):\n",
    "                if col != 'Code':\n",
    "                    # Calculate rolling 14 days variance to be put in y_lag_df\n",
    "                    y_lag_df[col] = price_df.loc[:, col].rolling(14).var()\n",
    "                    # Shift this col up by 13 days to be put in y_df (future predictions)\n",
    "                    y_df[col] = y_lag_df[col].shift(-14)\n",
    "                    # Calculate the error by subtracting the current price by  previous 14-day MA\n",
    "                    err = price_df.loc[:, col] -  price_df.loc[:, col].rolling(14).mean().shift(1)\n",
    "                    # Calculate the sign of error term (indicator r.v.)\n",
    "                    err_dir = err.apply(lambda x: 1 if x > 0 else 0)\n",
    "                    # Calculate square of error to be another variable in error_sq_df\n",
    "                    error_sq_df[col] = err ** 2\n",
    "                    # Calculate the Indicator*error_sq\n",
    "                    error_ind_df[col] = error_sq_df[col] * err_dir\n",
    "                    \n",
    "            \n",
    "            sub_dict['price'] = price_df\n",
    "            sub_dict['vol'] = vol_df\n",
    "            sub_dict['var'] = y_df\n",
    "            sub_dict['var_lag'] = y_lag_df\n",
    "            sub_dict['error_sq'] = error_sq_df\n",
    "            sub_dict['error_ind'] = error_ind_df\n",
    "            compile_list.append(sub_dict)\n",
    "            \n",
    "        return_list.append(compile_list)\n",
    "        \n",
    "    return return_list\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Rearrange(sector_list):\n",
    "    \"\"\"\n",
    "    Input: sector_list (return list from GenerateFeatures function above)\n",
    "    Return: list of list of dict as follows\n",
    "        [2014, 2015, 2016, ...] -> {key: PTT, data: dataframes}\n",
    "        (similar how we usually render in React)\n",
    "    \"\"\"\n",
    "    # Each sublist is for each year\n",
    "    return_list = [[], [], [], [], [], []]\n",
    "    \n",
    "    # i runs from 0 to # of sectors - 1\n",
    "    for i in range(len(sector_list)):\n",
    "        sector_stocks = sector_list[i]\n",
    "        # j runs from 0 to 5 (0 -> 2014, 5 -> 2019)\n",
    "        for j in range(6):\n",
    "            sector_year_dict = sector_stocks[j]\n",
    "            col_names = sector_year_dict['price'].columns.tolist()\n",
    "            # k runs for each column except \"Code\"\n",
    "            for k in range(1, len(col_names)):\n",
    "                stock_name = col_names[k]\n",
    "                # Step 1: create blank dataframes\n",
    "                new_data = pd.DataFrame(index=sector_year_dict['price'].index, columns= ['Code', 'vol', 'price', 'lag_y', 'error_sq', 'error_signed', 'y'])\n",
    "                new_data['Code'] = sector_year_dict['price']['Code']\n",
    "                # Step 2: Retrieve data from the key and insert it into new_data\n",
    "                new_data['vol'] = sector_year_dict['vol'].loc[:, stock_name]\n",
    "                new_data['price'] = sector_year_dict['price'].loc[:, stock_name]\n",
    "                new_data['lag_y'] = sector_year_dict['var_lag'].loc[:, stock_name]\n",
    "                new_data['error_sq'] = sector_year_dict['error_sq'].loc[:, stock_name]\n",
    "                new_data['error_signed'] = sector_year_dict['error_ind'].loc[:, stock_name]\n",
    "                new_data['y'] = sector_year_dict['var'].loc[:, stock_name]\n",
    "                # Step 3: Drop rows with at least one missing value\n",
    "                new_data.dropna(axis = 0, how = 'any', inplace = True)\n",
    "                # Step 4: Insert the new_dataframe into appropriate place (notice we use dict for convenience later)\n",
    "                new_dict = {'key': stock_name, 'data': new_data}\n",
    "                return_list[j].append(new_dict)\n",
    "                \n",
    "    return return_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetVolumeTraded(RenderList):\n",
    "    \"\"\"\n",
    "    input: RenderList, using the same data structure corresponding to Rearrange function (defined above)\n",
    "    Process: Add key of volume_traded in each stock-year (calculated from last year)\n",
    "    Return: nothing\n",
    "    \"\"\"\n",
    "    # For each year from 2015 to 2019\n",
    "    for i in range(1, 6):\n",
    "        \n",
    "        # For each stock that we want to add the new key: volume traded\n",
    "        for stock in RenderList[i]:\n",
    "            stock_name = stock['key']\n",
    "            \n",
    "            prev_data = RenderList[i-1]\n",
    "            \n",
    "            # This is inefficient, but it's okay, because otherwise, we need to change the whole data structure\n",
    "            for prev_stock in prev_data:\n",
    "                if prev_stock['key'] == stock_name:\n",
    "                    # Calculate the latest volume in last year\n",
    "                    stock['volume_traded'] = prev_stock['data'].loc[:, 'vol'].iloc[-1]\n",
    "                    \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RemoveNoKey(RenderList, key):\n",
    "    remove_keys = []\n",
    "    \n",
    "    for entry in RenderList:\n",
    "        if key not in entry:\n",
    "            remove_keys.append(entry[\"key\"]) # Append stock abbreviations we want to remove\n",
    "            \n",
    "    if len(remove_keys) == 0:\n",
    "        return RenderList\n",
    "    else:\n",
    "        return [x for x in RenderList if x[\"key\"] not in remove_keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def GenerateTable(RenderList):\n",
    "    \"\"\"\n",
    "    Input: RenderList, using the same data structure corresponding to Rearrange function and has the key volume_traded\n",
    "    This will output the result of average coefficient across five years into csv file\n",
    "    Return: nothing\n",
    "    \"\"\"\n",
    "    # This is the return dataframe with three columns corresponding to each coefficient\n",
    "    return_df = pd.DataFrame(index=range(10) , columns= ['lag_y', 'error_sq', 'error_signed'])\n",
    "    return_df = return_df.fillna(0)\n",
    "    \n",
    "    for i in range(1, 6):\n",
    "        ## For debug purpose\n",
    "        print('We are at year ' + str(i))\n",
    "        # Sort by volume traded and split into ten sub-lists\n",
    "        stock_list = RemoveNoKey(RenderList[i], 'volume_traded')\n",
    "        stock_list = sorted(stock_list, key = lambda x: x['volume_traded'])\n",
    "        ten_splits = np.array_split(stock_list, 10)\n",
    "        for j in range(10):\n",
    "            ## For debug purpose\n",
    "            print('---> We are at decile of ' + str(j) + ' consisting of '+ str(len(ten_splits[j])) + ' firms!')\n",
    "            \n",
    "            x1, x2, x3 = 0, 0, 0\n",
    "            \n",
    "            for Render in ten_splits[j]:\n",
    "                combined_df = Render['data']\n",
    "                combined_df.drop(['Code', 'vol', 'price'], axis = 1, inplace = True)\n",
    "                X = combined_df[['lag_y', 'error_sq', 'error_signed']]\n",
    "                y = combined_df['y']\n",
    "                \n",
    "                model = LinearRegression().fit(X, y)\n",
    "                model_coef = model.coef_\n",
    "                \n",
    "                lag_y, error_sq, error_signed = model_coef[0], model_coef[1], model_coef[2]\n",
    "                x1 = x1 + lag_y\n",
    "                x2 = x2 + error_sq\n",
    "                x3 = x3 + error_signed\n",
    "                \n",
    "            return_df.loc[j, 'lag_y'] = return_df.loc[j, 'lag_y'] + (x1/len(ten_splits[j]))\n",
    "            return_df.loc[j, 'error_sq'] = return_df.loc[j, 'error_sq'] + (x2/len(ten_splits[j]))\n",
    "            return_df.loc[j, 'error_signed'] = return_df.loc[j, 'error_signed'] + (x3/len(ten_splits[j]))\n",
    "                \n",
    "            #combined_df = pd.concat([x['data'] for x in ten_splits[j]])\n",
    "           #combined_df.drop(['Code', 'vol', 'price'], axis = 1, inplace = True)\n",
    "            #X = combined_df[['lag_y', 'error_sq', 'error_signed']]\n",
    "            #y = combined_df['y']\n",
    "            \n",
    "            # Run multiple linear regression here\n",
    "            #model = LinearRegression().fit(X, y)\n",
    "            #model_coef = model.coef_\n",
    "            \n",
    "            # Insert model_coef in return_df\n",
    "            #lag_y, error_sq, error_signed = model_coef[0], model_coef[1], model_coef[2]\n",
    "            #return_df.loc[j, 'lag_y'] = return_df.loc[j, 'lag_y'] + lag_y\n",
    "            #return_df.loc[j, 'error_sq'] = return_df.loc[j, 'error_sq'] + error_sq\n",
    "            #return_df.loc[j, 'error_signed'] = return_df.loc[j, 'error_signed'] + error_signed\n",
    "            \n",
    "    # Get average value\n",
    "    return_df['lag_y'] = return_df['lag_y']/5\n",
    "    return_df['error_sq'] = return_df['error_sq']/5\n",
    "    return_df['error_signed'] = return_df['error_signed']/5\n",
    "    \n",
    "    return_df.to_csv('Model_V1_Result_3.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3997: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are at year 1\n",
      "---> We are at decile of 0 consisting of 27 firms!\n",
      "---> We are at decile of 1 consisting of 27 firms!\n",
      "---> We are at decile of 2 consisting of 27 firms!\n",
      "---> We are at decile of 3 consisting of 27 firms!\n",
      "---> We are at decile of 4 consisting of 27 firms!\n",
      "---> We are at decile of 5 consisting of 26 firms!\n",
      "---> We are at decile of 6 consisting of 26 firms!\n",
      "---> We are at decile of 7 consisting of 26 firms!\n",
      "---> We are at decile of 8 consisting of 26 firms!\n",
      "---> We are at decile of 9 consisting of 26 firms!\n",
      "We are at year 2\n",
      "---> We are at decile of 0 consisting of 25 firms!\n",
      "---> We are at decile of 1 consisting of 25 firms!\n",
      "---> We are at decile of 2 consisting of 25 firms!\n",
      "---> We are at decile of 3 consisting of 25 firms!\n",
      "---> We are at decile of 4 consisting of 25 firms!\n",
      "---> We are at decile of 5 consisting of 25 firms!\n",
      "---> We are at decile of 6 consisting of 24 firms!\n",
      "---> We are at decile of 7 consisting of 24 firms!\n",
      "---> We are at decile of 8 consisting of 24 firms!\n",
      "---> We are at decile of 9 consisting of 24 firms!\n",
      "We are at year 3\n",
      "---> We are at decile of 0 consisting of 34 firms!\n",
      "---> We are at decile of 1 consisting of 34 firms!\n",
      "---> We are at decile of 2 consisting of 33 firms!\n",
      "---> We are at decile of 3 consisting of 33 firms!\n",
      "---> We are at decile of 4 consisting of 33 firms!\n",
      "---> We are at decile of 5 consisting of 33 firms!\n",
      "---> We are at decile of 6 consisting of 33 firms!\n",
      "---> We are at decile of 7 consisting of 33 firms!\n",
      "---> We are at decile of 8 consisting of 33 firms!\n",
      "---> We are at decile of 9 consisting of 33 firms!\n",
      "We are at year 4\n",
      "---> We are at decile of 0 consisting of 33 firms!\n",
      "---> We are at decile of 1 consisting of 33 firms!\n",
      "---> We are at decile of 2 consisting of 33 firms!\n",
      "---> We are at decile of 3 consisting of 33 firms!\n",
      "---> We are at decile of 4 consisting of 33 firms!\n",
      "---> We are at decile of 5 consisting of 33 firms!\n",
      "---> We are at decile of 6 consisting of 32 firms!\n",
      "---> We are at decile of 7 consisting of 32 firms!\n",
      "---> We are at decile of 8 consisting of 32 firms!\n",
      "---> We are at decile of 9 consisting of 32 firms!\n",
      "We are at year 5\n",
      "---> We are at decile of 0 consisting of 38 firms!\n",
      "---> We are at decile of 1 consisting of 38 firms!\n",
      "---> We are at decile of 2 consisting of 38 firms!\n",
      "---> We are at decile of 3 consisting of 38 firms!\n",
      "---> We are at decile of 4 consisting of 38 firms!\n",
      "---> We are at decile of 5 consisting of 38 firms!\n",
      "---> We are at decile of 6 consisting of 38 firms!\n",
      "---> We are at decile of 7 consisting of 38 firms!\n",
      "---> We are at decile of 8 consisting of 38 firms!\n",
      "---> We are at decile of 9 consisting of 37 firms!\n"
     ]
    }
   ],
   "source": [
    "cleaned_dfs = GenerateFeatures(PRICE_FILES, VOLUME_FILES)\n",
    "RenderList = Rearrange(cleaned_dfs)\n",
    "GetVolumeTraded(RenderList)\n",
    "GenerateTable(RenderList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lag_y</th>\n",
       "      <th>error_sq</th>\n",
       "      <th>error_signed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.020773</td>\n",
       "      <td>0.031523</td>\n",
       "      <td>-0.023770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.136868</td>\n",
       "      <td>-0.033102</td>\n",
       "      <td>0.148192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.015657</td>\n",
       "      <td>-0.029460</td>\n",
       "      <td>0.144527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.352925</td>\n",
       "      <td>0.001045</td>\n",
       "      <td>-0.073513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.004538</td>\n",
       "      <td>-0.004142</td>\n",
       "      <td>0.032461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.034629</td>\n",
       "      <td>-0.036425</td>\n",
       "      <td>0.021066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.081072</td>\n",
       "      <td>-0.004273</td>\n",
       "      <td>0.127749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.001646</td>\n",
       "      <td>-0.039256</td>\n",
       "      <td>0.125487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      lag_y  error_sq  error_signed\n",
       "0  0.020773  0.031523     -0.023770\n",
       "1  0.136868 -0.033102      0.148192\n",
       "2 -0.015657 -0.029460      0.144527\n",
       "3  0.352925  0.001045     -0.073513\n",
       "4 -0.004538 -0.004142      0.032461\n",
       "5 -0.034629 -0.036425      0.021066\n",
       "6  0.081072 -0.004273      0.127749\n",
       "7 -0.001646 -0.039256      0.125487\n",
       "8  0.000000  0.000000      0.000000\n",
       "9  0.000000  0.000000      0.000000"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## This is the format of excel file that you should get. (This is not the real one; just for debugging)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was a lot to be explained as we have to run generic code to find suitable data structure to encapsulate all stocks and aggregate the results by using generic function to loop through all cleaned stocks! Also, I have to debug a lot when I come across this model.   \n",
    "To sum up, what we should get is two excel files: Model_V1_Result.csv (combining all stocks in that decile into one model) and Model_V1_Result_3.csv (separately running LR on each stock and aggregate the result). In the next part, we will visualize the result as we did in part 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
